{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport tensorflow as tf\nimport pandas as pd\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.processors import TemplateProcessing\nfrom transformers import PreTrainedTokenizerFast\nimport datasets\nfrom datasets import load_dataset\nfrom transformers import Seq2SeqTrainer,Seq2SeqTrainingArguments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-21T22:05:14.204451Z","iopub.execute_input":"2023-01-21T22:05:14.204820Z","iopub.status.idle":"2023-01-21T22:05:22.485437Z","shell.execute_reply.started":"2023-01-21T22:05:14.204788Z","shell.execute_reply":"2023-01-21T22:05:22.484211Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig, AutoModelForSeq2SeqLM\n\nmodel_checkpoint = \"el-profesor/bert_small_seq2seq\"\nconfig = AutoConfig.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_config(config)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:05:36.064784Z","iopub.execute_input":"2023-01-21T22:05:36.065886Z","iopub.status.idle":"2023-01-21T22:05:38.187038Z","shell.execute_reply.started":"2023-01-21T22:05:36.065852Z","shell.execute_reply":"2023-01-21T22:05:38.185762Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8e6dd037e7408bbdfff8687c2e9ce8"}},"metadata":{}}]},{"cell_type":"code","source":"!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip python.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_long_list = ['repo', 'path', 'url', 'code', \n                     'code_tokens', 'docstring', 'docstring_tokens', \n                     'language', 'partition']\n\ncolumns_short_list = ['code_tokens', 'docstring_tokens', \n                      'language', 'partition']\n\ndef jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n    return pd.concat([pd.read_json(f, \n                                   orient='records', \n                                   compression='gzip',\n                                   lines=True)[columns] \n                      for f in file_list], sort=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:06:53.319220Z","iopub.execute_input":"2023-01-21T22:06:53.319832Z","iopub.status.idle":"2023-01-21T22:06:53.327368Z","shell.execute_reply.started":"2023-01-21T22:06:53.319797Z","shell.execute_reply":"2023-01-21T22:06:53.326243Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\npython_files = sorted(Path('./python/').glob('**/*.gz'))\npydf = jsonl_list_to_dataframe(python_files)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:06:57.366557Z","iopub.execute_input":"2023-01-21T22:06:57.366939Z","iopub.status.idle":"2023-01-21T22:07:37.243030Z","shell.execute_reply.started":"2023-01-21T22:06:57.366907Z","shell.execute_reply":"2023-01-21T22:07:37.242028Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\nfast_tokenizer = Tokenizer.from_pretrained(model_checkpoint)\ntokenizer = PreTrainedTokenizerFast(tokenizer_object=fast_tokenizer)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:07:39.018189Z","iopub.execute_input":"2023-01-21T22:07:39.018575Z","iopub.status.idle":"2023-01-21T22:07:39.433806Z","shell.execute_reply.started":"2023-01-21T22:07:39.018543Z","shell.execute_reply":"2023-01-21T22:07:39.432621Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"train_df = pd.DataFrame(pydf[pydf['partition']=='train'],columns=['code','docstring'])\ntrain_df.to_csv('train.csv',index=False)\nvalid_df = pd.DataFrame(pydf[pydf['partition']=='valid'],columns=['code','docstring'])\nvalid_df.to_csv('valid.csv',index=False)\ntest_df = pd.DataFrame(pydf[pydf['partition']=='test'],columns=['code','docstring'])\ntest_df.to_csv('test.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:07:44.063556Z","iopub.execute_input":"2023-01-21T22:07:44.063928Z","iopub.status.idle":"2023-01-21T22:07:56.989084Z","shell.execute_reply.started":"2023-01-21T22:07:44.063896Z","shell.execute_reply":"2023-01-21T22:07:56.987732Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data_files = {\"train\": \"train.csv\",\"valid\":\"valid.csv\", \"test\": \"test.csv\"}\ndataset = load_dataset(path = '/kaggle/working',data_files=data_files)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:08:02.264998Z","iopub.execute_input":"2023-01-21T22:08:02.265649Z","iopub.status.idle":"2023-01-21T22:08:11.294115Z","shell.execute_reply.started":"2023-01-21T22:08:02.265609Z","shell.execute_reply":"2023-01-21T22:08:11.292968Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/working to /root/.cache/huggingface/datasets/csv/working-c1f91967e6fdd5c6/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8743084f1d46d0839f709407db7381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73cd0c12efc4a0694ab1bbb0c0dfa83"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/working-c1f91967e6fdd5c6/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f78b025bb5455fb0a602bb2ca0327b"}},"metadata":{}}]},{"cell_type":"code","source":"# remove docstring from code\ndef clean_code_column(examples):\n    list_ = []\n    for e in examples['code']:\n        eg = e\n        triple_double = eg.split('\"\"\"')\n        if len(triple_double)==3:\n            eg = triple_double[0]+triple_double[-1]\n        triple_single = eg.split(\"'''\")\n        if len(triple_single)==3:\n            eg = triple_single[0]+triple_single[-1]\n        single_double = eg.split('\"')\n        if len(single_double)==3:\n            eg = single_double[0]+single_double[-1]\n        single_single = eg.split(\"'\")\n        if len(single_single)==3:\n            eg = single_single[0]+single_single[-1]\n        list_.append(eg)\n    examples['code']=list_\n    return examples\ndataset = dataset.map(clean_code_column, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:08:13.226844Z","iopub.execute_input":"2023-01-21T22:08:13.227248Z","iopub.status.idle":"2023-01-21T22:08:22.384310Z","shell.execute_reply.started":"2023-01-21T22:08:13.227215Z","shell.execute_reply":"2023-01-21T22:08:22.383279Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/413 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0ff80ce08a4ff6afc6453dce9b9685"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/24 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c07b4134984e43a5d2a7771bccb88b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/23 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c95a860a9db4f468e16e1ceb3572cb0"}},"metadata":{}}]},{"cell_type":"code","source":"encoder_max_length=512\ndecoder_max_length=128\n\ndef process_data_to_model_inputs(batch):\n  # tokenize the inputs and labels\n  inputs = tokenizer(batch[\"code\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n  outputs = tokenizer(batch[\"docstring\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n\n  batch[\"input_ids\"] = inputs.input_ids\n  batch[\"attention_mask\"] = inputs.attention_mask\n  batch[\"decoder_input_ids\"] = outputs.input_ids\n  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n  batch[\"labels\"] = outputs.input_ids.copy()\n\n  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n  # We have to make sure that the PAD token is ignored\n  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n\n  return batch","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:08:37.411881Z","iopub.execute_input":"2023-01-21T22:08:37.412274Z","iopub.status.idle":"2023-01-21T22:08:37.420714Z","shell.execute_reply.started":"2023-01-21T22:08:37.412240Z","shell.execute_reply":"2023-01-21T22:08:37.419396Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"batch_size=1000\n\ntrain_data = dataset['train'].map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size, \n    remove_columns=[\"code\", \"docstring\"]\n)\nval_data = dataset['valid'].map(\n    process_data_to_model_inputs, \n    batched=True, \n    batch_size=batch_size, \n    remove_columns=[\"code\", \"docstring\"]\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:08:42.007588Z","iopub.execute_input":"2023-01-21T22:08:42.007971Z","iopub.status.idle":"2023-01-21T22:14:56.260512Z","shell.execute_reply.started":"2023-01-21T22:08:42.007939Z","shell.execute_reply":"2023-01-21T22:14:56.259244Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/413 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77bddec5201c47799227249861a04112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/24 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d93038d95944ba9949e4122527f104d"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:19:23.603312Z","iopub.execute_input":"2023-01-21T22:19:23.604382Z","iopub.status.idle":"2023-01-21T22:19:36.546621Z","shell.execute_reply.started":"2023-01-21T22:19:23.604339Z","shell.execute_reply":"2023-01-21T22:19:36.545139Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge_score) (0.15.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge_score) (3.8.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.21.6)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.15.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (4.64.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge_score) (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (4.1.1)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=2d7e54e6b4b85a0ecb294c46287bf8c5b10bf6d3a046709705720f69da708732\n  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"batch_size=64\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True, \n    output_dir=\"./\",\n    logging_steps=200,\n    save_steps=200,\n    eval_steps=10000,\n    warmup_steps=2000,\n    weight_decay=0.01,\n    save_total_limit=3,\n)\n\nrouge = datasets.load_metric(\"rouge\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n\n    return {\n        \"rouge2_precision\": round(rouge_output.precision, 4),\n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n    }","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:19:38.026523Z","iopub.execute_input":"2023-01-21T22:19:38.026917Z","iopub.status.idle":"2023-01-21T22:19:38.276281Z","shell.execute_reply.started":"2023-01-21T22:19:38.026882Z","shell.execute_reply":"2023-01-21T22:19:38.275275Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-01-21T22:19:42.438709Z","iopub.execute_input":"2023-01-21T22:19:42.439104Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 412178\n  Num Epochs = 3\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 19323\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230121_222108-ckkp834g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/el-profesor/huggingface/runs/ckkp834g\" target=\"_blank\">./</a></strong> to <a href=\"https://wandb.ai/el-profesor/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12885' max='19323' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12885/19323 6:02:20 < 3:01:04, 0.59 it/s, Epoch 2.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge2 Precision</th>\n      <th>Rouge2 Recall</th>\n      <th>Rouge2 Fmeasure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10000</td>\n      <td>0.001400</td>\n      <td>0.002874</td>\n      <td>0.000100</td>\n      <td>0.000400</td>\n      <td>0.000100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./checkpoint-200\nConfiguration saved in ./checkpoint-200/config.json\nModel weights saved in ./checkpoint-200/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-400\nConfiguration saved in ./checkpoint-400/config.json\nModel weights saved in ./checkpoint-400/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-600\nConfiguration saved in ./checkpoint-600/config.json\nModel weights saved in ./checkpoint-600/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-800\nConfiguration saved in ./checkpoint-800/config.json\nModel weights saved in ./checkpoint-800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-1000\nConfiguration saved in ./checkpoint-1000/config.json\nModel weights saved in ./checkpoint-1000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-1200\nConfiguration saved in ./checkpoint-1200/config.json\nModel weights saved in ./checkpoint-1200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-1400\nConfiguration saved in ./checkpoint-1400/config.json\nModel weights saved in ./checkpoint-1400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-1600\nConfiguration saved in ./checkpoint-1600/config.json\nModel weights saved in ./checkpoint-1600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-1800\nConfiguration saved in ./checkpoint-1800/config.json\nModel weights saved in ./checkpoint-1800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-1200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-2000\nConfiguration saved in ./checkpoint-2000/config.json\nModel weights saved in ./checkpoint-2000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-1400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-2200\nConfiguration saved in ./checkpoint-2200/config.json\nModel weights saved in ./checkpoint-2200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-1600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-2400\nConfiguration saved in ./checkpoint-2400/config.json\nModel weights saved in ./checkpoint-2400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-1800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-2600\nConfiguration saved in ./checkpoint-2600/config.json\nModel weights saved in ./checkpoint-2600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-2800\nConfiguration saved in ./checkpoint-2800/config.json\nModel weights saved in ./checkpoint-2800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-2200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-3000\nConfiguration saved in ./checkpoint-3000/config.json\nModel weights saved in ./checkpoint-3000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-2400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-3200\nConfiguration saved in ./checkpoint-3200/config.json\nModel weights saved in ./checkpoint-3200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-2600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-3400\nConfiguration saved in ./checkpoint-3400/config.json\nModel weights saved in ./checkpoint-3400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-2800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-3600\nConfiguration saved in ./checkpoint-3600/config.json\nModel weights saved in ./checkpoint-3600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-3800\nConfiguration saved in ./checkpoint-3800/config.json\nModel weights saved in ./checkpoint-3800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-3200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-4000\nConfiguration saved in ./checkpoint-4000/config.json\nModel weights saved in ./checkpoint-4000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-3400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-4200\nConfiguration saved in ./checkpoint-4200/config.json\nModel weights saved in ./checkpoint-4200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-3600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-4400\nConfiguration saved in ./checkpoint-4400/config.json\nModel weights saved in ./checkpoint-4400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-3800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-4600\nConfiguration saved in ./checkpoint-4600/config.json\nModel weights saved in ./checkpoint-4600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-4800\nConfiguration saved in ./checkpoint-4800/config.json\nModel weights saved in ./checkpoint-4800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-4200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-5000\nConfiguration saved in ./checkpoint-5000/config.json\nModel weights saved in ./checkpoint-5000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-4400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-5200\nConfiguration saved in ./checkpoint-5200/config.json\nModel weights saved in ./checkpoint-5200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-4600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-5400\nConfiguration saved in ./checkpoint-5400/config.json\nModel weights saved in ./checkpoint-5400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-4800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-5600\nConfiguration saved in ./checkpoint-5600/config.json\nModel weights saved in ./checkpoint-5600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-5800\nConfiguration saved in ./checkpoint-5800/config.json\nModel weights saved in ./checkpoint-5800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-5200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-6000\nConfiguration saved in ./checkpoint-6000/config.json\nModel weights saved in ./checkpoint-6000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-5400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-6200\nConfiguration saved in ./checkpoint-6200/config.json\nModel weights saved in ./checkpoint-6200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-5600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-6400\nConfiguration saved in ./checkpoint-6400/config.json\nModel weights saved in ./checkpoint-6400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-5800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-6600\nConfiguration saved in ./checkpoint-6600/config.json\nModel weights saved in ./checkpoint-6600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-6000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-6800\nConfiguration saved in ./checkpoint-6800/config.json\nModel weights saved in ./checkpoint-6800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-6200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-7000\nConfiguration saved in ./checkpoint-7000/config.json\nModel weights saved in ./checkpoint-7000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-6400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-7200\nConfiguration saved in ./checkpoint-7200/config.json\nModel weights saved in ./checkpoint-7200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-6600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-7400\nConfiguration saved in ./checkpoint-7400/config.json\nModel weights saved in ./checkpoint-7400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-6800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-7600\nConfiguration saved in ./checkpoint-7600/config.json\nModel weights saved in ./checkpoint-7600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-7000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-7800\nConfiguration saved in ./checkpoint-7800/config.json\nModel weights saved in ./checkpoint-7800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-7200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-8000\nConfiguration saved in ./checkpoint-8000/config.json\nModel weights saved in ./checkpoint-8000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-7400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-8200\nConfiguration saved in ./checkpoint-8200/config.json\nModel weights saved in ./checkpoint-8200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-7600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-8400\nConfiguration saved in ./checkpoint-8400/config.json\nModel weights saved in ./checkpoint-8400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-7800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-8600\nConfiguration saved in ./checkpoint-8600/config.json\nModel weights saved in ./checkpoint-8600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-8000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-8800\nConfiguration saved in ./checkpoint-8800/config.json\nModel weights saved in ./checkpoint-8800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-8200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-9000\nConfiguration saved in ./checkpoint-9000/config.json\nModel weights saved in ./checkpoint-9000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-8400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-9200\nConfiguration saved in ./checkpoint-9200/config.json\nModel weights saved in ./checkpoint-9200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-8600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-9400\nConfiguration saved in ./checkpoint-9400/config.json\nModel weights saved in ./checkpoint-9400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-8800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-9600\nConfiguration saved in ./checkpoint-9600/config.json\nModel weights saved in ./checkpoint-9600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-9000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-9800\nConfiguration saved in ./checkpoint-9800/config.json\nModel weights saved in ./checkpoint-9800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-9200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n***** Running Evaluation *****\n  Num examples = 23107\n  Batch size = 64\nwandb: Network error (ReadTimeout), entering retry loop.\nSaving model checkpoint to ./checkpoint-10000\nConfiguration saved in ./checkpoint-10000/config.json\nModel weights saved in ./checkpoint-10000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-9400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-10200\nConfiguration saved in ./checkpoint-10200/config.json\nModel weights saved in ./checkpoint-10200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-9600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-10400\nConfiguration saved in ./checkpoint-10400/config.json\nModel weights saved in ./checkpoint-10400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-9800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-10600\nConfiguration saved in ./checkpoint-10600/config.json\nModel weights saved in ./checkpoint-10600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-10000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-10800\nConfiguration saved in ./checkpoint-10800/config.json\nModel weights saved in ./checkpoint-10800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-10200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-11000\nConfiguration saved in ./checkpoint-11000/config.json\nModel weights saved in ./checkpoint-11000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-10400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-11200\nConfiguration saved in ./checkpoint-11200/config.json\nModel weights saved in ./checkpoint-11200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-10600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-11400\nConfiguration saved in ./checkpoint-11400/config.json\nModel weights saved in ./checkpoint-11400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-10800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-11600\nConfiguration saved in ./checkpoint-11600/config.json\nModel weights saved in ./checkpoint-11600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-11000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-11800\nConfiguration saved in ./checkpoint-11800/config.json\nModel weights saved in ./checkpoint-11800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-11200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-12000\nConfiguration saved in ./checkpoint-12000/config.json\nModel weights saved in ./checkpoint-12000/pytorch_model.bin\nDeleting older checkpoint [checkpoint-11400] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-12200\nConfiguration saved in ./checkpoint-12200/config.json\nModel weights saved in ./checkpoint-12200/pytorch_model.bin\nDeleting older checkpoint [checkpoint-11600] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-12400\nConfiguration saved in ./checkpoint-12400/config.json\nModel weights saved in ./checkpoint-12400/pytorch_model.bin\nDeleting older checkpoint [checkpoint-11800] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-12600\nConfiguration saved in ./checkpoint-12600/config.json\nModel weights saved in ./checkpoint-12600/pytorch_model.bin\nDeleting older checkpoint [checkpoint-12000] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nSaving model checkpoint to ./checkpoint-12800\nConfiguration saved in ./checkpoint-12800/config.json\nModel weights saved in ./checkpoint-12800/pytorch_model.bin\nDeleting older checkpoint [checkpoint-12200] due to args.save_total_limit\n/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.cuda.memory_summary(device=None, abbreviated=False))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}